\documentclass[12pt,a4paper]{article}
\usepackage{ssn-me-cse-review}
\usepackage{float}
\usepackage{url}
\usepackage{alltt}
\usepackage{longtable}
%\usepackage{mathtools}
\usepackage[document]{ragged2e}
\usepackage{algorithm2e}[1]
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{trees}

\begin{document}
\ptitle{Textual Entailment Recognition on Large Datasets}
\review{1}
\student{ Subalakshmi Shanthosi S (186001008)}
\semester{3}
\guide{Dr. Aravindan Chandrabose }
\reviewdate{14 August 2019}
\reviewtitle
\hrule

\section{Introduction}
The intent of Textual entailment is to identify whether one piece of text can be plausibly inferred from another – It is a major generic core problem in Natural Language Understanding(NLU).\\
The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several areas, such as question answering, information retrieval, information extraction and document summarization.\\
Textual Entailment is predominantly dependent on high quality,huge annotated corpus. However, until now, the scarcity of such data on one hand, and the costs of creating new datasets of reasonable size on the other, have represented a bottleneck for a steady advancement
towards achieving the state-of-the-art performance.\\
Crowdsourcing services have been recently used for creation of NLP resources is that the acquisition and annotation of large datasets, needed to train and evaluate NLP tools and applications, can be carried out in a cost-effective manner.\\
The accuracy of pretrained model decreases on increase in the size of dataset used for training.This counter-intuitive result is due to Hetrogenity and increased corpus size. 
\section{Motivation}
In Natural language processing field, Recognising textual entailment (RTE) is of \\paramount importance.All the complex NLU problems have discern entailment as it's sub NLP task. Image recognition and Computer Vision see rely on RTE to improve results obtained applying visual techniques alone.\\
Applications dealing with text needs a semantic framework for applied semantics and RTE may provide such framework.\\
\newpage
Textual Entailment is used for modelling language variability in NLP Tasks which are given below.\\
$\bullet$ Variability of semantic expression : Same meaning can be inferred from different texts.\\	
$\bullet$ Ambiguity in meaning of words : Different meanings can be inferred from same text.
\\
Textual entailment is also used for Machine Translation Evaluation.Applying such entailment phenomenon on MT evaluation provides the well formedness of the output sentence
generated by the translation system.\\
Textual entailment (TE) in natural language processing is a directional relation between text fragments.The relation is directional because even if "t entails h", the reverse "h entails t" is much less uncertain.\\

Role of Knowledge source for TE is very crucial as success of the entailment system heavily depends
on the background knowledge. Background knowledge includes facts, conventions, peculiar language features such as certain metaphors, idioms, proverbs,
common beliefs etc. 
Challenges in Textual Entailment:
\\~\\
$\bullet$ Paraphrasing: Same meaning can be inferred from different texts.\\	
T and H contains same fact but expressed with different words.\\
Example: "the cat devours the mouse"
              is a paraphrase of 
          "the cat consumes the mouse" \\
$\bullet$ Strict Entailment: T and H carry same facts such that one can be inferred from another.\\
Example: Yahoo bought Overture and Yahoo owns Overture.	

\section{Problem statement}
Given an collection of annotated tweets using an annotation model that encompasses following levels indicating the depth of detail on Offensive Language Identification and Categorisation task.Our goal is to find the presence of offensive language and the severity of its existance during impact assessment. \\~\\
The three shared tasks are as follows:
\begin{enumerate}
	\item Sub-task A - Offensive language identification
	\item Sub-task B - Automatic categorization of offense types
	\item Sub-task C - Offense target identification.
\end{enumerate}

\section{Literature survey}
\subsection{SSN\_NLP at SemEval-2019 Task 6: Offensive Language Identification in Social Media using Traditional and Deep Machine Learning Approaches\cite{ssnNLPSemEvalT6}}
In this paper,OLID\cite{zampierietal2019} dataset is used which contains a collection of annotated tweets and an annotation indicating the level of Offensive language severity assesment and categorisation done.Traditional Machine Learning and Deep Learning techniques are employed to identify offensive languages.
Deep Learning methods uses Bi-LSTM to derive vectorised tweets and uses attention mechanism to map the offensive slang words to a named a named entity - "GRP".In Traditional Machine Learning approach feature vectors are constructed using TF-IDF scoring and Multinomial Naive Bayes (MNB) and Support Vector Machine (SVM) with Stochastic Gradient Descent optimizer to build the models. 
 
\subsection{An Exploration of State-of-the-art Methods for Offensive Language Detection\cite{stateofartSemEvalT6}}
In this paper,the proposed system works on OLID dataset which is partially preprocessed to annotate user as @USER and URL's as URL.Words are transformed to lower case and removing alphanumeric symbols leaving behind only letters,digits and underscore as acceptable characters.\\
 Word2Vec is used for generating word embeddings. Rather than using pre-trained models such as scraped Wikipedia pages, a combination of transforming vectors is used for generating multi-word single vector for further processing.\\
Auto-Keras was used to train a pre-trained BERT representation.But,BERT regards capitalized and syntactically incorrect statements as noise thus failing to categorise the level of abusive nature in that particular sentence.\\
 FastText trained with random search for fine tuning the existing pretrained model with scraped Wikipedia pages even when it is modelled to work on large datasets. 
 \subsection{Recognition of Partial Textual Entailment for Indian Social Media Text\cite{partialTESocial}}
 Partial Textual Entailment in NLP is used for defining partial entailment relationship between T-H pair.Thus,PTE plays an important role in different NLP applications like Text summarisation and Question answering by reducing redundant information.\\
 In this paper,contributions are as follows:
 \begin{enumerate}
 	\item Extending classical TE by including two categories of partial entailment for Bengali tweets.
 	\item Manual creation of PTE annotation on Bengali Social Media Text corpus.The corpus includes total 5916 numbers of tweet pairs.
 \end{enumerate}
Emperical Definition of Partial Entailment:
Defines four categories of PTE.
\begin{enumerate}
	\item PTE-I : Preserverance of original entailment relationship and the relationship is bi-directional.
	i.e) H entails from T and T entails from H.
	\item PTE-II : This category has two conditions to follow.
	\begin{itemize}
		\item Condition I:If H entails from the whole
		meaning of T and have additional information,
		then it is a category of PTE-II and represents
		as,\\ \begin{center}
			(X\textsubscript{H} Entails from T) + Y\textsubscript{H}
		\end{center}
		\item Condition II: If T entails from the whole
		meaning of H and have additional information,
		then it is also a category of PTE-II and
		represents as,
		\begin{center}
			(X\textsubscript{T} Entails from H) + Y\textsubscript{T}
		\end{center}
	\end{itemize} 
	\item PTE-III: If a portion of H entails from a portion
	of T or vice verse, then it is a category of
	PTE-III and represents as,\\
	\begin{center}
		(X\textsubscript{H} Entails from T) + Y\textsubscript{H}\\
		(X\textsubscript{T} Entails from H) + Y\textsubscript{T}
	\end{center}
\item PTE-IV: If T or H does not entail from H or T,
then its a category of PTE-IV and represents
as Non-entailed.
\end{enumerate}
Sequential Minimal Optimization(SMO) based PTE recognition approach is used on Social Media Text for partial matching.
Future work is to make this system robust to handle code-mixed tweets.
\subsection{ Absit invidia verbo: Comparing Deep Learning methods for offensive language\cite{comparingDeepLearning}}
Bag-of-words model is used as dataset initially and then word2idx for neural network model.\\ 
Extensive use of PyTorch , Keras, scikit-learn, and Natural Language Toolkit(NLTK) is observed. PyTorch is selected to implement CNN, Keras for RNN and Linear Regression using scikit-learn. For Offensive language identification Logistic Regression,LSTM and B-LSTM outperforms other models.\\
 Each tasks is trained with 90\% of samples and 10\% of samples are used for testing.L2 regularisation is used for optimising results.
\subsection{Benchmarking Aggression Identification in Social Media\cite{benchmarkAggressionIden}}
In this work,a dataset of 15,000 aggression-annotated Facebook Posts and Comments each in Hindi (in both Roman and Devanagari script) and English are provided for training and validation. For testing, two different sets - one from Facebook and another from a different social media - were provided.
This paper reports the results of the first Shared Task on Aggression Identification which was organised
jointly with the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018.
The aim of this shared task is Classification of Social Media Text as overt aggression, covert aggression and nonaggression.
The dataset considered has subjective inaccurate annotation and contains code-mixed texts which are noisy and need to be carefully filtered.\\
Multilingual lexicon of aggressive words. The lexicon is obtained by automatic translation from an handmade lexicon of offensive words in Italian, with minimal human supervision. The original words are expanded into a list of their senses. The senses are manually annotated to filter out senses that are
never used in an offensive context.
Even LSTM pretained FastText vector performed better than conventional Neural network models.
\newpage
\section{Existing system}
	 The existing approaches have used Deep Learning and pre-trained models like BERT,FastText,CNN or Conventional Machine Learning techniques like Naive Bayes and Stochastic Gradient Descent for identification and categorisation of Offensive language in Social media texts\cite{ssnNLPSemEvalT6}\cite{stateofartSemEvalT6}.\\
 Partial Textual Entailment can be used for Offensive Language Categorisation as it aims at finding SMO for partial matching and reducing reduntant information.\cite{partialTESocial}\\
 Deep learning approach have been predominantly used and shows promising results even on Multi-Lingual datasets.\cite{benchmarkAggressionIden}. 	

\section{Proposed system}
  In our work, the offensive language usage can be identified in social media text by defining PTE rules by using Sequential Minimal Optimization(SMO) method.\\
  Increasing the dataset population by using Semantic textual similarity for determining paraphrases of offensive slang sentences.\\
  Finding means to incorporate Transfer Learning approach by using pre-trained models like XLM ,BERT and XLNet. 

\begin{thebibliography}{99}
	\bibliographystyle{plain}
	
	\bibitem[1]{ssnNLPSemEvalT6}D. Thenmozhi, B. Senthil Kumar, Chandrabose Aravindan, S.Srinethe\\{\em SSN NLP at SemEval-2019 Task 6: Offensive Language Identification in Social Media using Traditional and Deep Machine Learning Approaches.}Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019) 739 -- 744,2019.	
	
	\bibitem[2]{stateofartSemEvalT6}Harrison Uglow,Martin Zlocha,Szymon Zmyslony,{\em An Exploration of State-of-the-art Methods for Offensive Language Detection.}arXiv:1903.07445 1-- 5,2019.

	\newpage

	\bibitem[3]{partialTESocial}Dwijen Rudrapal , Amitava Das  , Baby Bhattacharya ,{\em Recognition of Partial Textual Entailment for Indian Social Media Text.}Computación y Sistemas, Year 23, Vol. 23  143 -- 152,2019.
	
	\bibitem[4]{comparingDeepLearning} Bogdan Lazarescu, Christo Lolov , Silvia Sapora, {\em Absit invidia verbo: Comparing Deep Learning methods for offensive language.},arXiv:1903.05929v3 1-- 5,2019.
	
	\bibitem[5]{benchmarkAggressionIden}Ritesh Kumar
	, Atul Kr. Ojha , Shervin Malmasi , Marcos Zampieri ,{\em Benchmarking Aggression Identification in Social Media} ,Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying,  1 -- 11 , 2018.
	\bibitem[6]{zampierietal2019} Zampieri, Marcos and Malmasi, Shervin and Nakov, Preslav and Rosenthal, Sara and Farra, Noura and Kumar, Ritesh,{\em Predicting the Type and Target of Offensive Posts in Social Media} ,  Proceedings of NAACL,2019.
\end{thebibliography}
	
\end{document}
